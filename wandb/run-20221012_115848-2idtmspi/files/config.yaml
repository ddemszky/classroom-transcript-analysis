wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.13.4
    framework: huggingface
    huggingface_version: 4.23.1
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.10.6
    start_time: 1665601128.500089
    t:
      1:
      - 1
      - 5
      - 11
      - 31
      - 49
      - 51
      - 53
      - 55
      2:
      - 1
      - 5
      - 11
      - 31
      - 49
      - 51
      - 53
      - 55
      3:
      - 1
      - 16
      - 23
      4: 3.10.6
      5: 0.13.4
      6: 4.23.1
      8:
      - 4
      - 5
      9:
        2: simpletransformers
adafactor_beta1:
  desc: null
  value: null
adafactor_clip_threshold:
  desc: null
  value: 1.0
adafactor_decay_rate:
  desc: null
  value: -0.8
adafactor_eps:
  desc: null
  value:
  - 1.0e-30
  - 0.001
adafactor_relative_step:
  desc: null
  value: true
adafactor_scale_parameter:
  desc: null
  value: true
adafactor_warmup_init:
  desc: null
  value: true
adam_betas:
  desc: null
  value:
  - 0.9
  - 0.999
adam_epsilon:
  desc: null
  value: 1.0e-08
best_model_dir:
  desc: null
  value: outputs/roberta/student_on_task/best_model
cache_dir:
  desc: null
  value: outputs/roberta/student_on_task/cache
config:
  desc: null
  value: {}
cosine_schedule_num_cycles:
  desc: null
  value: 0.5
custom_layer_parameters:
  desc: null
  value: []
custom_parameter_groups:
  desc: null
  value: []
dataloader_num_workers:
  desc: null
  value: 0
do_lower_case:
  desc: null
  value: false
dynamic_quantize:
  desc: null
  value: false
early_stopping_consider_epochs:
  desc: null
  value: false
early_stopping_delta:
  desc: null
  value: 0
early_stopping_metric:
  desc: null
  value: eval_loss
early_stopping_metric_minimize:
  desc: null
  value: true
early_stopping_patience:
  desc: null
  value: 3
encoding:
  desc: null
  value: null
eval_batch_size:
  desc: null
  value: 8
evaluate_during_training:
  desc: null
  value: true
evaluate_during_training_silent:
  desc: null
  value: true
evaluate_during_training_steps:
  desc: null
  value: 500
evaluate_during_training_verbose:
  desc: null
  value: false
evaluate_each_epoch:
  desc: null
  value: true
fp16:
  desc: null
  value: false
gradient_accumulation_steps:
  desc: null
  value: 4
labels_list:
  desc: null
  value:
  - 0
  - 1
labels_map:
  desc: null
  value: {}
lazy_delimiter:
  desc: null
  value: "\t"
lazy_labels_column:
  desc: null
  value: 1
lazy_loading:
  desc: null
  value: false
lazy_loading_start_line:
  desc: null
  value: 1
lazy_text_a_column:
  desc: null
  value: null
lazy_text_b_column:
  desc: null
  value: null
lazy_text_column:
  desc: null
  value: 0
learning_rate:
  desc: null
  value: 4.0e-05
local_rank:
  desc: null
  value: -1
logging_steps:
  desc: null
  value: 50
loss_args:
  desc: null
  value: {}
loss_type:
  desc: null
  value: null
manual_seed:
  desc: null
  value: null
max_grad_norm:
  desc: null
  value: 1.0
max_seq_length:
  desc: null
  value: 512
model_class:
  desc: null
  value: ClassificationModel
model_name:
  desc: null
  value: roberta-base
model_type:
  desc: null
  value: roberta
multiprocessing_chunksize:
  desc: null
  value: -1
n_gpu:
  desc: null
  value: 1
no_cache:
  desc: null
  value: false
no_save:
  desc: null
  value: false
not_saved_args:
  desc: null
  value: []
num_train_epochs:
  desc: null
  value: 5
onnx:
  desc: null
  value: false
optimizer:
  desc: null
  value: AdamW
output_dir:
  desc: null
  value: outputs/roberta/student_on_task
overwrite_output_dir:
  desc: null
  value: true
polynomial_decay_schedule_lr_end:
  desc: null
  value: 1.0e-07
polynomial_decay_schedule_power:
  desc: null
  value: 1.0
process_count:
  desc: null
  value: 8
quantized_model:
  desc: null
  value: false
regression:
  desc: null
  value: false
reprocess_input_data:
  desc: null
  value: true
save_best_model:
  desc: null
  value: true
save_eval_checkpoints:
  desc: null
  value: false
save_model_every_epoch:
  desc: null
  value: false
save_optimizer_and_scheduler:
  desc: null
  value: true
save_steps:
  desc: null
  value: 2000
scheduler:
  desc: null
  value: linear_schedule_with_warmup
silent:
  desc: null
  value: false
skip_special_tokens:
  desc: null
  value: true
sliding_window:
  desc: null
  value: false
special_tokens_list:
  desc: null
  value: []
stride:
  desc: null
  value: 0.8
tensorboard_dir:
  desc: null
  value: outputs/roberta/student_on_task/tensorboard
thread_count:
  desc: null
  value: null
tie_value:
  desc: null
  value: 1
tokenizer_name:
  desc: null
  value: roberta-base
tokenizer_type:
  desc: null
  value: null
train_batch_size:
  desc: null
  value: 4
train_custom_parameters_only:
  desc: null
  value: false
use_cached_eval_features:
  desc: null
  value: false
use_early_stopping:
  desc: null
  value: false
use_hf_datasets:
  desc: null
  value: false
use_multiprocessing:
  desc: null
  value: true
use_multiprocessing_for_evaluation:
  desc: null
  value: true
wandb_kwargs:
  desc: null
  value:
    reinit: true
wandb_project:
  desc: null
  value: student_on_task
warmup_ratio:
  desc: null
  value: 0.06
warmup_steps:
  desc: null
  value: 44
weight_decay:
  desc: null
  value: 0.0
